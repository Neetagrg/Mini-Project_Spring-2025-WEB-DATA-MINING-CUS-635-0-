{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9L3x6/3A5+D2EOjEyyF/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neetagrg/Mini-Project_Spring-2025-WEB-DATA-MINING-CUS-635-0-/blob/main/miniproject635.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini Project: Neeta Kumari & Bir Bahadur Gharti"
      ],
      "metadata": {
        "id": "AwwY6hDi2ANj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iCI9KCGtQfH",
        "outputId": "e388b48c-58c0-47b3-85cf-b2efd46d5d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.37.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.11 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.37.11)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.11.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 requests pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Import the Required Libraries\n",
        "We need to import boto3, requests (for fetching data from the API), and other libraries to handle the data."
      ],
      "metadata": {
        "id": "Y-KVBagatdun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "import requests\n",
        "import pandas as pd\n",
        "from botocore.config import Config\n",
        "from botocore import UNSIGNED\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHRseMr-tmRY",
        "outputId": "967e0e83-1fa1-4303-c99d-c6a44346ed53"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the AWS S3 Connection"
      ],
      "metadata": {
        "id": "Z6XTGDmVtpMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Team 6 folder\n",
        "TEAM = \"TEAM_6/\"\n",
        "\n",
        "# Define the provided S3 bucket name\n",
        "BUCKET_NAME = \"cus635-spring2025\"\n",
        "\n",
        "# Create an anonymous S3 client\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n"
      ],
      "metadata": {
        "id": "mBzTTmm5uR8z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetching Articles from the Guardian API"
      ],
      "metadata": {
        "id": "_dVD31dCuWmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "PAGE_SIZE = 200  # Maximum per request\n",
        "total_articles = []\n",
        "\n",
        "# Function to fetch articles and handle errors\n",
        "def fetch_articles(page_num):\n",
        "    params = {\n",
        "        'api-key': API_KEY,\n",
        "        'section': 'technology',\n",
        "        'q': 'AI in health',  # Search query\n",
        "        'page-size': PAGE_SIZE,\n",
        "        'page': page_num,  # Change page number\n",
        "        'show-fields': 'body'  # Request content (body) of the article\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(API_URL, params=params)\n",
        "        response.raise_for_status()  # Will raise an error for bad status codes\n",
        "        return response.json()  # Return the JSON response\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching page {page_num}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to process and store articles\n",
        "def process_articles(articles_data):\n",
        "    articles = articles_data['response']['results']\n",
        "    for article in articles:\n",
        "        heading = article['webTitle']  # Extracting the title\n",
        "        context = article['fields'].get('body', 'No content available')  # Extracting the content\n",
        "\n",
        "        # Clean context: truncate if it's too long and remove unnecessary whitespace\n",
        "        cleaned_context = (context[:500] + '...') if len(context) > 500 else context.strip()\n",
        "\n",
        "        total_articles.append({\n",
        "            'heading': heading,\n",
        "            'context': cleaned_context\n",
        "        })\n",
        "\n",
        "# Fetch and process articles\n",
        "for page_num in tqdm(range(1, 6), desc=\"Fetching pages\", unit=\"page\"):\n",
        "    articles_data = fetch_articles(page_num)\n",
        "    if articles_data:\n",
        "        process_articles(articles_data)\n",
        "    else:\n",
        "        print(f\"Skipping page {page_num} due to an error.\")\n",
        "\n",
        "# Create a DataFrame for better presentation and export\n",
        "df = pd.DataFrame(total_articles)\n",
        "\n",
        "# Optionally save the results to a CSV file for later use\n",
        "df.to_csv(\"AI_in_Health_Articles.csv\", index=False)\n",
        "\n",
        "# Displaying first 5 articles for quick preview\n",
        "print(df.head())\n",
        "\n",
        "print(f\"Fetched {len(total_articles)} articles across 5 pages.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG0yBZ7au3A7",
        "outputId": "61b59585-d0e9-4adf-8a00-c6326c7484f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|██████████| 5/5 [00:06<00:00,  1.24s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             heading  \\\n",
            "0  Prioritise artists over tech in AI copyright d...   \n",
            "1  EU accused of leaving ‘devastating’ copyright ...   \n",
            "2  AI ‘godfather’ predicts another revolution in ...   \n",
            "3  If the best defence against AI is more AI, thi...   \n",
            "4  Chinese AI chatbot DeepSeek censors itself in ...   \n",
            "\n",
            "                                             context  \n",
            "0  <p>Two cross-party committees of MPs have urge...  \n",
            "1  <p>An architect of EU copyright law has said l...  \n",
            "2  <p>One of the “godfathers” of modern artificia...  \n",
            "3  <p>Oscar Wilde’s quip, “Life imitates art far ...  \n",
            "4  <p>Users experimenting with DeepSeek have seen...  \n",
            "Fetched 1000 articles across 5 pages.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Articles as a CSV"
      ],
      "metadata": {
        "id": "XswnAAk_veWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_articles_to_csv(articles, filename):\n",
        "    article_data = []\n",
        "\n",
        "    for article in articles:\n",
        "        title = article['webTitle']\n",
        "        url = article['webUrl']\n",
        "        category = article['health-related']\n",
        "        content = article.get('fields', {}).get('bodyText', 'No content available')\n",
        "\n",
        "        # Append article details to the list\n",
        "        article_data.append([title, url, category, content])\n",
        "\n",
        "    # Convert the list into a DataFrame and save to CSV\n",
        "    df = pd.DataFrame(article_data, columns=['Title', 'URL', 'Category', 'Content'])\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Articles saved to {filename}\")\n"
      ],
      "metadata": {
        "id": "fA9wtmyKvpz1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload the CSV to S3"
      ],
      "metadata": {
        "id": "n5iWnLV2vwgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def upload_file_to_s3(local_filename, s3_filename):\n",
        "    try:\n",
        "        s3.upload_file(local_filename, BUCKET_NAME, TEAM + s3_filename)\n",
        "        print(f\"File '{local_filename}' uploaded successfully to {TEAM} folder in the S3 bucket.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file {local_filename} was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file: {e}\")\n"
      ],
      "metadata": {
        "id": "s_XDEzHZv1iM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download a File from S3"
      ],
      "metadata": {
        "id": "45EgxXIhv9l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def download_file_from_s3(s3_filename, local_filename):\n",
        "    try:\n",
        "        s3.download_file(BUCKET_NAME, TEAM + s3_filename, local_filename)\n",
        "        print(f\"File '{s3_filename}' downloaded successfully as '{local_filename}'!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n"
      ],
      "metadata": {
        "id": "bPluyBvgv_Tq"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}